{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d892720d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import json \n",
    "import pandas as pd \n",
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "with open('token.json', 'r') as f:\n",
    "    auth_data = json.load(open('token.json', 'r'))\n",
    "\n",
    "headers = {\n",
    "    'Authorization' : 'Bearer ' + auth_data['access_token']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f85feae",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnx = mysql.connector.connect(user=config.user, password=config.password, host=config.host, database=config.database)\n",
    "cursor = cnx.cursor()\n",
    "\n",
    "# getting anime_id and get information using api call\n",
    "def get_anime_id():\n",
    "    anime_name = input(\"Enter an anime name \\n\")\n",
    "    #using get request to get a json response\n",
    "    anime_response = requests.get(f\"https://api.myanimelist.net/v2/anime?q={anime_name}&limit=10\", headers=headers)\n",
    "    anime_data = anime_response.json()\n",
    "\n",
    "    anime_names = [anime_data['data'][anime]['node']['title'] for anime in range(len(anime_data['data']))]\n",
    "    for i, anime_name in enumerate(anime_names):   \n",
    "        print(f\"{i+1}. {anime_name}\")\n",
    "    \n",
    "    # asking user input to type an anime and selecting the anime they specify (shows 10 results)\n",
    "    get_id = int(input(\"Please select specific Anime by it's number on the left: \"))\n",
    "\n",
    "    if get_id in range(1, 11):\n",
    "        selected_anime = anime_names[get_id - 1]\n",
    "        print(f\"Selected {selected_anime}\")\n",
    "    else:\n",
    "        print(\"Enter a number.\")\n",
    "    \n",
    "    #using json beautifier to get the data from the response \n",
    "    anime_id = anime_data['data'][get_id - 1]['node']['id'] \n",
    "\n",
    "    if 'data' not in anime_data or len(anime_data['data']) == 0:\n",
    "        print(\"Error: No anime found. \")\n",
    "\n",
    "    response = requests.get(f\"https://api.myanimelist.net/v2/anime/{anime_id}?fields=id,title,start_date,genres,mean\", headers=headers)\n",
    "\n",
    "    anime = response.json()\n",
    "\n",
    "    #getting data using the json data (tags)\n",
    "    title = anime['title']\n",
    "    anime_id = anime['id']\n",
    "    airing_date = anime['start_date']\n",
    "    genres = ', '.join([genre['name'] for genre in anime['genres']])\n",
    "    score = anime['mean']\n",
    "\n",
    "    #make a query to using sql commands\n",
    "    query = \"INSERT INTO anime (anime_id, name, year_of_release, genre, score) VALUES (%s, %s, %s, %s, %s) ON DUPLICATE KEY UPDATE score=VALUES(score)\"\n",
    "    values = (anime_id, title, airing_date, genres, score)\n",
    "    cursor.execute(query, values)\n",
    "    cnx.commit()\n",
    "\n",
    "    # Fetch and print the results\n",
    "    for result in cursor:\n",
    "        print(result)\n",
    "        \n",
    "    return anime_id \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97366a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to get the forum_ids from the data we got from get_anime_id\n",
    "def get_forum_ids_scraper(anime_id):\n",
    "    all_t_id = []\n",
    "    shows = 0\n",
    "    page_num = 1\n",
    "    \n",
    "    while True:\n",
    "        # Construct the URL for the current page number\n",
    "        url = f\"https://myanimelist.net/forum/?animeid={anime_id}&topic=episode&show={shows}\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        rows = soup.find_all('tr', {'id': lambda x: x and x.startswith('topicRow')})\n",
    "        \n",
    "        if not rows:\n",
    "            break\n",
    "        \n",
    "        t_id = [row['data-topic-id'] for row in rows]\n",
    "        print(f\"Forum IDs for page {page_num}: {t_id}\")\n",
    "        all_t_id.extend(t_id)\n",
    "        shows += 50\n",
    "        page_num += 1\n",
    "        \n",
    "    return all_t_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad020da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(forum_ids):\n",
    "    limit = 100\n",
    "\n",
    "    bodies = []\n",
    "    comment_ids = []\n",
    "\n",
    "    for ids in forum_ids:\n",
    "        offset = 0\n",
    "        while True:\n",
    "            response = requests.get(f\"https://api.myanimelist.net/v2/forum/topic/{ids}?limit={limit}&offset={offset}\", headers=headers)\n",
    "            forum_data = response.json()\n",
    "            posts = forum_data['data']['posts']\n",
    "            if not posts:\n",
    "                break\n",
    "            bodies += [item['body'] for item in posts]\n",
    "            comment_id = forum_data['data']['posts'][0]['id']\n",
    "            comment_ids += [item['id'] for item in posts]\n",
    "            if len(posts) < limit:\n",
    "                break\n",
    "            offset += limit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3855fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close the cursor and connection\n",
    "cursor.close()\n",
    "cnx.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15914ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = [html.unescape(re.sub(r'\\[yt\\].*?\\[/yt\\]|\\[img\\].*?\\[/img\\]|\\[.*?\\]|\\t|\\n|\\r|\\xa0|<[^<]+?>', '', post['body'])) for post in forum_data['data']['posts']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c88b21d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(f\"https://api.myanimelist.net/v2/forum/topic/{forum_id}?limit={limit}&offset={offset}\", headers=headers)\n",
    "forum_data = response.json()\n",
    "#posts = [html.unescape(re.sub(r'\\[yt\\].*?\\[/yt\\]|\\[img\\].*?\\[/img\\]|\\[.*?\\]|\\t|\\n|\\r|\\xa0|<[^<]+?>', '', post['body'])) for post in forum_data['data']['posts']]\n",
    "print(posts)\n",
    "posts = forum_data['data']['posts']\n",
    "if not posts:\n",
    "    break\n",
    "comment_ids = [post['id'] for post in posts]\n",
    "bodies = [post['body'] for post in posts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e15163",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mysql.connector\n",
    "import requests\n",
    "import json\n",
    "# database information in config.py\n",
    "import config\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from text_process import text_process\n",
    "import re\n",
    "import html\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e7df7ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter an anime name \n",
      "bleach\n",
      "1. Bleach Movie 4: Jigoku-hen\n",
      "2. Bleach Movie 3: Fade to Black - Kimi no Na wo Yobu\n",
      "3. Bleach Movie 1: Memories of Nobody\n",
      "4. Bleach\n",
      "5. Bleach Movie 2: The DiamondDust Rebellion - Mou Hitotsu no Hyourinmaru\n",
      "6. Bleach: Sennen Kessen-hen\n",
      "7. Bleach: Sennen Kessen-hen - Ketsubetsu-tan\n",
      "8. Bleach: The Sealed Sword Frenzy\n",
      "9. Bleach: Memories in the Rain\n",
      "10. Bleach KaraBuri!: Gotei Juusan Yatai Daisakusen!\n",
      "Please select specific Anime by it's number on the left: 4\n",
      "Selected Bleach\n",
      "Forum IDs for page 1: ['418789', '415821', '413289', '410963', '408513', '405931', '402785', '399713', '396855', '393407', '390537', '387075', '381069', '378427', '374637', '372289', '370093', '367935', '365629', '363345', '360665', '358207', '354597', '351457', '351367', '332145', '329059', '326037', '323635', '321265', '319087', '316842', '314551', '312095', '309515', '307331', '305451', '304225', '303114', '302092', '301042', '300038', '299002', '298044', '297094', '296077', '295265', '294112', '293096', '292130']\n",
      "Forum IDs for page 2: ['291182', '290316', '289374', '288662', '287796', '286966', '286060', '285209', '284397', '283158', '282237', '281344', '280342', '279424', '277324', '276243', '275177', '273873', '272713', '271645', '270400', '269241', '268022', '266825', '265716', '264562', '263193', '261768', '260385', '259186', '257775', '256489', '254963', '253376', '251740', '249949', '248104', '246323', '244422', '242174', '233912', '231788', '217180', '213720', '211691', '209589', '207762', '205950', '203871', '201915']\n",
      "Forum IDs for page 3: ['199879', '198055', '194216', '170717', '168118', '166402', '163670', '161914', '160129', '158074', '156368', '154116', '152306', '150778', '148963', '145199', '143498', '141598', '135736', '134121', '132229', '130513', '128699', '126901', '124215', '123369', '121506', '119680', '118044', '115630', '113491', '111369', '109334', '107531', '105560', '103838', '102127', '100678', '99287', '97398', '96005', '94352', '92874', '90689', '89240', '87267', '85984', '84834', '83615', '82185']\n",
      "Forum IDs for page 4: ['80766', '79210', '77968', '76520', '75329', '73724', '72340', '71696', '73270', '68362', '67115', '65919', '64747', '63530', '62448', '61260', '58855', '57787', '56648', '55337', '54292', '53351', '52231', '51249', '50126', '49057', '47916', '46649', '43713', '42727', '41651', '40513', '39384', '37099', '35943', '34887', '33886', '32802', '31860', '30991', '30989', '30046', '28965', '27873', '26824', '25756', '24895', '24109', '22450', '21494']\n",
      "Forum IDs for page 5: ['20038', '17024', '16283', '15596', '14812', '14091', '13357', '12832', '12270', '11785', '11240', '10717', '9804', '9421', '8990', '8614', '8133', '7774', '7442', '7103', '6808', '6507', '6220', '5659', '5391', '5111', '4823', '4568', '4351', '4088', '3533', '3012', '2846', '2678', '2486', '2356', '2187', '2059', '1929', '1788', '1584', '1494', '1393', '1269', '1150', '1022', '861', '802', '674', '661']\n",
      "Forum IDs for page 6: ['36247', '557', '458', '36212', '36206', '1684831', '1684830', '1684827', '1684826', '1684823', '35698', '97731', '35632', '35683', '35611', '35583', '35466', '35522', '35517', '35514', '97571', '35449', '97207', '34568', '34558', '35045', '34423', '34397', '34394', '34251', '34085', '33930', '97112', '97108', '33068', '97040', '89888', '96898', '96826', '96818', '96813', '96808', '96804', '16609', '96788', '96578', '78855', '93203', '93175', '93157']\n",
      "Forum IDs for page 7: ['93156', '78283', '21194', '93015', '93007', '93001', '92997', '92989', '56449', '38501', '38498', '38496', '38493', '15422', '55643', '38444', '38441', '38436', '38433', '38367', '38364', '38362', '38357', '38354', '38353', '38352', '38348', '38342', '38340', '38334', '38329', '38326', '38311', '38298', '38275', '38271', '38189', '38186', '38185', '38176', '38174', '38169', '38166', '38161', '38153', '38147', '38118', '38105', '38094', '38086']\n",
      "Forum IDs for page 8: ['38081', '90612', '37956', '37950', '37945', '88168', '37943', '37927', '37922', '37915', '37907', '15774', '15618', '37817', '37811', '37810']\n",
      "  forum_id  comment_id                                            message  \\\n",
      "0   418789    14004065  THIS IS AN ANIME ONLY DISCUSSION POST. DO NOT ...   \n",
      "1   418789    14004103  JUST finished it.Was a bit diffrent from the m...   \n",
      "2   418789    14004203  IchigoHollowfi said:Where did you watch 366???...   \n",
      "3   418789    14004287                Good riddance. Miss you I will not.   \n",
      "4   418789    14004321                               And so it ends..4/10   \n",
      "\n",
      "                                     cleaned_message  sentiment_score  \n",
      "0  anime discussion post discuss manga beyond epi...           0.9631  \n",
      "1  finished bit diffrent manga still awesome god ...           0.5859  \n",
      "2  ichigohollowfi said watch 366 watched live pro...           0.0000  \n",
      "3                                 good riddance miss           0.3182  \n",
      "4                                            ends 10           0.0000  \n"
     ]
    }
   ],
   "source": [
    "# asks for user input, an anime they want to search\n",
    "anime_name = input(\"Enter an anime name \\n\")\n",
    "anime_response = requests.get(f\"https://api.myanimelist.net/v2/anime?q={anime_name}&limit=10\", headers=headers)\n",
    "anime_data = anime_response.json()\n",
    "\n",
    "anime_names = [anime_data['data'][anime]['node']['title'] for anime in range(len(anime_data['data']))]\n",
    "for i, anime_name in enumerate(anime_names):   \n",
    "    print(f\"{i+1}. {anime_name}\")\n",
    "\n",
    "# asking user input to type an anime and selecting the anime they specify (shows 10 results)\n",
    "get_id = int(input(\"Please select specific Anime by it's number on the left: \"))\n",
    "\n",
    "if get_id in range(1, 11):\n",
    "    selected_anime = anime_names[get_id - 1]\n",
    "    print(f\"Selected {selected_anime}\")\n",
    "else:\n",
    "    print(\"Enter a number.\")\n",
    "\n",
    "# getting anime_id and get information using api call\n",
    "def get_anime_id():    \n",
    "    #using json beautifier to get the data from the response \n",
    "    anime_id = anime_data['data'][get_id - 1]['node']['id'] \n",
    "\n",
    "    if 'data' not in anime_data or len(anime_data['data']) == 0:\n",
    "        print(\"Error: No anime found. \")\n",
    "\n",
    "    response = requests.get(f\"https://api.myanimelist.net/v2/anime/{anime_id}?fields=id,title,start_date,genres,mean\", headers=headers)\n",
    "    anime = response.json()\n",
    "\n",
    "    #getting data using the json data (tags)\n",
    "    title = anime['title']\n",
    "    anime_id = anime['id']\n",
    "    airing_date = anime['start_date']\n",
    "    genres = ', '.join([genre['name'] for genre in anime['genres']])\n",
    "    score = anime['mean']\n",
    "\n",
    "    #make a query to using sql commands\n",
    "        \n",
    "    return anime_id \n",
    "\n",
    "#function to get the forum_ids from the data we got from get_anime_id\n",
    "def get_forum_ids_scraper(anime_id):\n",
    "    all_t_id = []\n",
    "    shows = 0\n",
    "    page_num = 1\n",
    "    \n",
    "    while True:\n",
    "        # Construct the URL for the current page number\n",
    "        url = f\"https://myanimelist.net/forum/?animeid={anime_id}&topic=episode&show={shows}\"\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        rows = soup.find_all('tr', {'id': lambda x: x and x.startswith('topicRow')})\n",
    "        \n",
    "        if not rows:\n",
    "            break\n",
    "        \n",
    "        t_id = [row['data-topic-id'] for row in rows]\n",
    "        print(f\"Forum IDs for page {page_num}: {t_id}\")\n",
    "        all_t_id.extend(t_id)\n",
    "        # because it shows 50 forum topics per page \n",
    "        shows += 50\n",
    "        page_num += 1\n",
    "        \n",
    "    return all_t_id\n",
    "\n",
    "\n",
    "#function to get data for forum_ids table \n",
    "def get_forum_id():\n",
    "    anime_id = get_anime_id()\n",
    "    forum_ids = get_forum_ids_scraper(anime_id)\n",
    "    for ids in forum_ids: \n",
    "        url = f\"https://api.myanimelist.net/v2/forum/topic/{ids}\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        forum_titles = response.json()\n",
    "        forum_title = forum_titles['data']['title']\n",
    "        \n",
    "\n",
    "def get_comments():\n",
    "    limit = 100\n",
    "    forum_ids = get_forum_ids_scraper(get_anime_id())\n",
    "    comments = {'forum_id': [], 'comment_id': [], 'message': [], 'cleaned_message': [], 'sentiment_score': []}\n",
    "\n",
    "    for forum_id in forum_ids:\n",
    "        offset = 0\n",
    "        while True:\n",
    "            response = requests.get(f\"https://api.myanimelist.net/v2/forum/topic/{forum_id}?limit={limit}&offset={offset}\", headers=headers)\n",
    "            forum_data = response.json()\n",
    "            posts = forum_data['data']['posts']\n",
    "            if not posts:\n",
    "                break\n",
    "            comment_ids = [post['id'] for post in posts]\n",
    "            bodies = [html.unescape(re.sub(r'\\[yt\\].*?\\[/yt\\]|\\[img\\].*?\\[/img\\]|\\[.*?\\]|\\t|\\n|\\r|\\xa0|<[^<]+?>', '', post['body'])) for post in posts]\n",
    "            cleaned_messages = [text_process(body) for body in bodies]\n",
    "            # Calculate sentiment score for each comment\n",
    "            sentiment_scores = [get_sentiment_score(text) for text in cleaned_messages]\n",
    "            comments['forum_id'].extend([forum_id] * len(posts))\n",
    "            comments['comment_id'].extend(comment_ids)\n",
    "            comments['message'].extend(bodies)\n",
    "            comments['cleaned_message'].extend(cleaned_messages)            \n",
    "            comments['sentiment_score'].extend(sentiment_scores)\n",
    "            if len(posts) < limit:\n",
    "                break\n",
    "            offset += limit\n",
    "\n",
    "    df = pd.DataFrame(comments)\n",
    "    print(df.head())\n",
    "    df.to_csv('cleaned_forum_data.csv', index=False)\n",
    "\n",
    "    # make query to select from comments and do analysis \n",
    "# Retrieve the data from the clean_message column in the comments table\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    # Convert the results to a Pandas DataFrame\n",
    "    df = pd.DataFrame(columns=['cleaned_message'])\n",
    "\n",
    "    # Define the get_sentiment_score function\n",
    "    sia = SentimentIntensityAnalyzer()\n",
    "    return sia.polarity_scores(str(text))['compound']\n",
    "\n",
    "\n",
    "def main():\n",
    "    get_comments()\n",
    "    # any other functions to execute\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
